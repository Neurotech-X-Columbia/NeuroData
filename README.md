# NeuroData
## Scripts for neural data collection, cloud storage, and retrieval
This repo contains documentation and utility scripts for data collection, cloud storage, and retrieval.

### **Workflows**

### Data collection workflow:
1. Design your data collection session. Prepare a stimulus script and location. You will probably need at least two laptops (one for data collection and one to provide stimulus).
2. Define session info JSON via collection script CLI. See ObjectInfo.md for more about info JSON files.
    - Session parameters: subject name, response type, stimulus type, block length, block count, and stim cycle
    - Hardware parameters: sampling rate, headset configuration, buffer size
    - Project name
    - Description
    - Date
    - Time
3. Proceed to collection. When complete, there will be a session folder containing the newly generated .csv file and its accompanying info file.
4. Supply info json and session .csv to the upload script to store data on the cloud.

### Data retrieval:
1. Use retrieval script to query S3, downloads into datapackage folder with the following structure.

datapackage/
    ├─ packageinfo.txt
    ├─ 24601/
    │  ├─ 24601_11-12-23.csv
    │  └─ info.json
    └─ 10025/
       ├─ 10024_11-13-25.csv
       └─ info.json

### **Functions**

### Database Functions:
1. Data retrieval by
    - project name
    - subject name
    - stimulus type
    - session date
    - response type
    - any combination of the above

### Storage Structure
S3 contains all session .csv files with unique names generated by upload script.
DynamoDB contains JSON info objects with references to S3 storage.
Retrieval script downloads data package from S3.
